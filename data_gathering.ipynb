{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import logging\n",
    "import os\n",
    "import jsonlines\n",
    "import sys\n",
    "import itertools\n",
    "from transformers import AutoTokenizer # For converting the texts into tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lamini\n",
    "lamini.api_url = os.getenv(\"POWERML__PRODUCTION__URL\")\n",
    "lamini.api_key = os.getenv(\"POWERML__PRODUCTION__KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up logging\n",
    "logger = logging.getLogger(__name__)\n",
    "def setup_logging():\n",
    "    FORMAT = '%(levelname)s:%(name)s: %(message)s (%(asctime)s; %(filename)s:%(lineno)d)'\n",
    "    DATE_FORMAT = '%Y-%m-%d %H:%M:%S'\n",
    "    LEVEL = logging.INFO\n",
    "    STREAM = sys.stdout\n",
    "    logging.basicConfig( \n",
    "        level=LEVEL, \n",
    "        format=FORMAT, \n",
    "        datefmt=DATE_FORMAT,\n",
    "        stream=STREAM,\n",
    "    )\n",
    "setup_logging()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization and Encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:__main__: [19352, 2505, 323, 5175, 10669, 1320] (2024-04-04 20:41:13; 1036835545.py:6)\n",
      "INFO:__main__: Sample text for testing tokenization (2024-04-04 20:41:13; 1036835545.py:10)\n"
     ]
    }
   ],
   "source": [
    "# Tokenising texts\n",
    "# string --encoding--> tokens\n",
    "tokenizer = AutoTokenizer.from_pretrained('EleutherAI/pythia-70m')\n",
    "text = \"Sample text for testing tokenization\"\n",
    "tokens = tokenizer(text)\n",
    "logger.info(tokens['input_ids'])\n",
    "\n",
    "# tokens --encoding--> string\n",
    "encoded_sentence = tokenizer.decode(tokens['input_ids'])\n",
    "logger.info(encoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[12764, 619, 1416, 310, 2006, 414, 66, 2790, 293, 15], [42, 1353, 247, 4382, 5859, 5974, 387, 13533, 28634, 2499, 15], [42, 1353, 22500, 670, 3694, 11369, 13, 5145, 4715, 15], [9917, 27799, 665, 14528, 7487, 7212, 581, 285, 25154, 15]]\n",
      "Using padding:  [[12764, 619, 1416, 310, 2006, 414, 66, 2790, 293, 15, 0], [42, 1353, 247, 4382, 5859, 5974, 387, 13533, 28634, 2499, 15], [42, 1353, 22500, 670, 3694, 11369, 13, 5145, 4715, 15, 0], [9917, 27799, 665, 14528, 7487, 7212, 581, 285, 25154, 15, 0]]\n",
      "Using truncation:  [[12764, 619, 1416, 310, 2006], [42, 1353, 247, 4382, 5859], [42, 1353, 22500, 670, 3694], [9917, 27799, 665, 14528, 7487]]\n",
      "Using left-side truncation:  [[414, 66, 2790, 293, 15], [387, 13533, 28634, 2499, 15], [11369, 13, 5145, 4715, 15], [7212, 581, 285, 25154, 15]]\n",
      "Using both padding and truncation:  [[414, 66, 2790, 293, 15], [387, 13533, 28634, 2499, 15], [11369, 13, 5145, 4715, 15], [7212, 581, 285, 25154, 15]]\n"
     ]
    }
   ],
   "source": [
    "list_texts = ['Hi my name is Aditya Patel.', 'I\\'m a computer science student at Toronto Metropolitan University.', 'I\\'m passionate about software engineering, machine learning.', 'Also athlete who loves watching formula one and cricket.']\n",
    "encoded_list = tokenizer(list_texts)\n",
    "print(encoded_list['input_ids'])\n",
    "\n",
    "# Padding Concept\n",
    "tokenizer.pad_token = tokenizer.eos_token \n",
    "encoded_texts_longest = tokenizer(list_texts, padding=True)\n",
    "print(\"Using padding: \", encoded_texts_longest[\"input_ids\"])\n",
    "\n",
    "# Truncation\n",
    "encoded_texts_truncation = tokenizer(list_texts, max_length=5, truncation=True)\n",
    "print(\"Using truncation: \", encoded_texts_truncation[\"input_ids\"])\n",
    "\n",
    "# Truncation Left\n",
    "tokenizer.truncation_side = \"left\"\n",
    "encoded_texts_truncation_left = tokenizer(list_texts, max_length=5, truncation=True)\n",
    "print(\"Using left-side truncation: \", encoded_texts_truncation_left[\"input_ids\"])\n",
    "\n",
    "# Truncation and Padding Together\n",
    "encoded_texts_both = tokenizer(list_texts, max_length=5, truncation=True, padding=True)\n",
    "print(\"Using both padding and truncation: \", encoded_texts_both[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instruction Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'instruction': 'Give three tips for staying healthy.', 'input': '', 'output': '1.Eat a balanced diet and make sure to include plenty of fruits and vegetables. \\n2. Exercise regularly to keep your body active and strong. \\n3. Get enough sleep and maintain a consistent sleep schedule.', 'text': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nGive three tips for staying healthy.\\n\\n### Response:\\n1.Eat a balanced diet and make sure to include plenty of fruits and vegetables. \\n2. Exercise regularly to keep your body active and strong. \\n3. Get enough sleep and maintain a consistent sleep schedule.'}\n",
      "\n",
      "{'instruction': 'What are the three primary colors?', 'input': '', 'output': 'The three primary colors are red, blue, and yellow.', 'text': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nWhat are the three primary colors?\\n\\n### Response:\\nThe three primary colors are red, blue, and yellow.'}\n",
      "\n",
      "{'instruction': 'Describe the structure of an atom.', 'input': '', 'output': 'An atom is made up of a nucleus, which contains protons and neutrons, surrounded by electrons that travel in orbits around the nucleus. The protons and neutrons have a positive charge, while the electrons have a negative charge, resulting in an overall neutral atom. The number of each particle determines the atomic number and the type of atom.', 'text': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nDescribe the structure of an atom.\\n\\n### Response:\\nAn atom is made up of a nucleus, which contains protons and neutrons, surrounded by electrons that travel in orbits around the nucleus. The protons and neutrons have a positive charge, while the electrons have a negative charge, resulting in an overall neutral atom. The number of each particle determines the atomic number and the type of atom.'}\n",
      "\n",
      "{'instruction': 'How can we reduce air pollution?', 'input': '', 'output': 'There are a number of ways to reduce air pollution, such as shifting to renewable energy sources, encouraging the use of public transportation, prohibiting the burning of fossil fuels, implementing policies to reduce emissions from industrial sources, and implementing vehicle emissions standards. Additionally, individuals can do their part to reduce air pollution by reducing car use, avoiding burning materials such as wood, and changing to energy efficient appliances.', 'text': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nHow can we reduce air pollution?\\n\\n### Response:\\nThere are a number of ways to reduce air pollution, such as shifting to renewable energy sources, encouraging the use of public transportation, prohibiting the burning of fossil fuels, implementing policies to reduce emissions from industrial sources, and implementing vehicle emissions standards. Additionally, individuals can do their part to reduce air pollution by reducing car use, avoiding burning materials such as wood, and changing to energy efficient appliances.'}\n",
      "\n",
      "{'instruction': 'Describe a time when you had to make a difficult decision.', 'input': '', 'output': 'I had to make a difficult decision when I was working as a project manager at a construction company. I was in charge of a project that needed to be completed by a certain date in order to meet the client’s expectations. However, due to unexpected delays, we were not able to meet the deadline and so I had to make a difficult decision. I decided to extend the deadline, but I had to stretch the team’s resources even further and increase the budget. Although it was a risky decision, I ultimately decided to go ahead with it to ensure that the project was completed on time and that the client’s expectations were met. The project was eventually successfully completed and this was seen as a testament to my leadership and decision-making abilities.', 'text': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nDescribe a time when you had to make a difficult decision.\\n\\n### Response:\\nI had to make a difficult decision when I was working as a project manager at a construction company. I was in charge of a project that needed to be completed by a certain date in order to meet the client’s expectations. However, due to unexpected delays, we were not able to meet the deadline and so I had to make a difficult decision. I decided to extend the deadline, but I had to stretch the team’s resources even further and increase the budget. Although it was a risky decision, I ultimately decided to go ahead with it to ensure that the project was completed on time and that the client’s expectations were met. The project was eventually successfully completed and this was seen as a testament to my leadership and decision-making abilities.'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Grabbing a data set form hugging face and making it reay for training\n",
    "instruction_tuned_dataset = datasets.load_dataset(\"tatsu-lab/alpaca\", split=\"train\", streaming=True)\n",
    "m = 5\n",
    "top_m = list(itertools.islice(instruction_tuned_dataset, m))\n",
    "for i in top_m:\n",
    "    print(i)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template_with_input = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Input:\n",
    "{input}\n",
    "\n",
    "### Response:\"\"\"\n",
    "\n",
    "prompt_template_without_input = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Response:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:__main__: {'input': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nGive three tips for staying healthy.\\n\\n### Response:', 'output': '1.Eat a balanced diet and make sure to include plenty of fruits and vegetables. \\n2. Exercise regularly to keep your body active and strong. \\n3. Get enough sleep and maintain a consistent sleep schedule.'} (2024-04-04 20:41:16; 2009408342.py:8)\n"
     ]
    }
   ],
   "source": [
    "processed_data = []\n",
    "for j in top_m:\n",
    "  if not j[\"input\"]:\n",
    "    processed_prompt = prompt_template_without_input.format(instruction=j[\"instruction\"])\n",
    "  else:\n",
    "    processed_prompt = prompt_template_with_input.format(instruction=j[\"instruction\"], input=j[\"input\"])\n",
    "  processed_data.append({\"input\": processed_prompt, \"output\": j[\"output\"]})\n",
    "logger.info(processed_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:__main__: File alpaca-instruction-tuning.jsonl successfully created. (2024-04-04 20:41:16; 1184919678.py:4)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    with jsonlines.open(\"alpaca-instruction-tuning.jsonl\", \"w\") as writer:\n",
    "        writer.write_all(processed_data)\n",
    "    logger.info(\"File alpaca-instruction-tuning.jsonl successfully created.\")\n",
    "except Exception as e:\n",
    "    logger.exception(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lamini Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:__main__: {'question': 'How can I evaluate the performance and quality of the generated text from Lamini models?', 'answer': \"There are several metrics that can be used to evaluate the performance and quality of generated text from Lamini models, including perplexity, BLEU score, and human evaluation. Perplexity measures how well the model predicts the next word in a sequence, while BLEU score measures the similarity between the generated text and a reference text. Human evaluation involves having human judges rate the quality of the generated text based on factors such as coherence, fluency, and relevance. It is recommended to use a combination of these metrics for a comprehensive evaluation of the model's performance.\", 'input_ids': [2347, 476, 309, 7472, 253, 3045, 285, 3290, 273, 253, 4561, 2505, 432, 418, 4988, 74, 3210, 32, 2512, 403, 2067, 17082, 326, 476, 320, 908, 281, 7472, 253, 3045, 285, 3290, 273, 4561, 2505, 432, 418, 4988, 74, 3210, 13, 1690, 44229, 414, 13, 378, 1843, 54, 4868, 13, 285, 1966, 7103, 15, 3545, 12813, 414, 5593, 849, 973, 253, 1566, 26295, 253, 1735, 3159, 275, 247, 3425, 13, 1223, 378, 1843, 54, 4868, 5593, 253, 14259, 875, 253, 4561, 2505, 285, 247, 3806, 2505, 15, 8801, 7103, 8687, 1907, 1966, 16006, 2281, 253, 3290, 273, 253, 4561, 2505, 1754, 327, 2616, 824, 347, 25253, 13, 2938, 1371, 13, 285, 17200, 15, 733, 310, 8521, 281, 897, 247, 5019, 273, 841, 17082, 323, 247, 11088, 7103, 273, 253, 1566, 434, 3045, 15], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [2347, 476, 309, 7472, 253, 3045, 285, 3290, 273, 253, 4561, 2505, 432, 418, 4988, 74, 3210, 32, 2512, 403, 2067, 17082, 326, 476, 320, 908, 281, 7472, 253, 3045, 285, 3290, 273, 4561, 2505, 432, 418, 4988, 74, 3210, 13, 1690, 44229, 414, 13, 378, 1843, 54, 4868, 13, 285, 1966, 7103, 15, 3545, 12813, 414, 5593, 849, 973, 253, 1566, 26295, 253, 1735, 3159, 275, 247, 3425, 13, 1223, 378, 1843, 54, 4868, 5593, 253, 14259, 875, 253, 4561, 2505, 285, 247, 3806, 2505, 15, 8801, 7103, 8687, 1907, 1966, 16006, 2281, 253, 3290, 273, 253, 4561, 2505, 1754, 327, 2616, 824, 347, 25253, 13, 2938, 1371, 13, 285, 17200, 15, 733, 310, 8521, 281, 897, 247, 5019, 273, 841, 17082, 323, 247, 11088, 7103, 273, 253, 1566, 434, 3045, 15]} (2024-04-04 20:41:18; 2534706368.py:7)\n",
      "INFO:__main__: {'question': \"Can I find information about the code's approach to handling long-running tasks and background jobs?\", 'answer': 'Yes, the code includes methods for submitting jobs, checking job status, and retrieving job results. It also includes a method for canceling jobs. Additionally, there is a method for sampling multiple outputs from a model, which could be useful for long-running tasks.', 'input_ids': [5804, 309, 1089, 1491, 670, 253, 2127, 434, 2746, 281, 10885, 1048, 14, 24220, 8892, 285, 4114, 7375, 32, 4374, 13, 253, 2127, 3797, 3082, 323, 29315, 7375, 13, 12669, 2628, 3708, 13, 285, 48484, 2628, 1543, 15, 733, 671, 3797, 247, 1332, 323, 14002, 272, 7375, 15, 9157, 13, 627, 310, 247, 1332, 323, 10491, 2709, 18012, 432, 247, 1566, 13, 534, 812, 320, 4217, 323, 1048, 14, 24220, 8892, 15], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [5804, 309, 1089, 1491, 670, 253, 2127, 434, 2746, 281, 10885, 1048, 14, 24220, 8892, 285, 4114, 7375, 32, 4374, 13, 253, 2127, 3797, 3082, 323, 29315, 7375, 13, 12669, 2628, 3708, 13, 285, 48484, 2628, 1543, 15, 733, 671, 3797, 247, 1332, 323, 14002, 272, 7375, 15, 9157, 13, 627, 310, 247, 1332, 323, 10491, 2709, 18012, 432, 247, 1566, 13, 534, 812, 320, 4217, 323, 1048, 14, 24220, 8892, 15]} (2024-04-04 20:41:18; 2534706368.py:7)\n",
      "INFO:__main__: {'question': 'How does Lamini AI handle requests for generating text that requires reasoning or decision-making based on given information?', 'answer': 'Lamini AI offers features for generating text that requires logical reasoning or inference beyond simple text generation. It can handle user prompts that involve complex reasoning or logical inference, and can generate text that captures the nuances of different cultural or regional variations.', 'input_ids': [2347, 1057, 418, 4988, 74, 14980, 6016, 9762, 323, 11365, 2505, 326, 4419, 14720, 390, 3061, 14, 11849, 1754, 327, 1677, 1491, 32, 45, 4988, 74, 14980, 6131, 3386, 323, 11365, 2505, 326, 4419, 13760, 14720, 390, 17032, 4457, 2969, 2505, 5978, 15, 733, 476, 6016, 2608, 49887, 326, 6388, 2570, 14720, 390, 13760, 17032, 13, 285, 476, 6635, 2505, 326, 28174, 253, 8794, 1972, 273, 1027, 8928, 390, 9933, 10575, 15], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [2347, 1057, 418, 4988, 74, 14980, 6016, 9762, 323, 11365, 2505, 326, 4419, 14720, 390, 3061, 14, 11849, 1754, 327, 1677, 1491, 32, 45, 4988, 74, 14980, 6131, 3386, 323, 11365, 2505, 326, 4419, 13760, 14720, 390, 17032, 4457, 2969, 2505, 5978, 15, 733, 476, 6016, 2608, 49887, 326, 6388, 2570, 14720, 390, 13760, 17032, 13, 285, 476, 6635, 2505, 326, 28174, 253, 8794, 1972, 273, 1027, 8928, 390, 9933, 10575, 15]} (2024-04-04 20:41:18; 2534706368.py:7)\n",
      "INFO:__main__: {'question': 'Does the `submit_job()` function expose any advanced training options such as learning rate schedules or early stopping?', 'answer': 'It is unclear which `submit_job()` function is being referred to as there is no such function defined in Lamini’s python library snippets. Please provide more information or context to answer the question accurately.', 'input_ids': [10795, 253, 2634, 21399, 64, 17455, 42702, 1159, 22065, 667, 7269, 3733, 4610, 824, 347, 4715, 2281, 28631, 390, 2393, 15910, 32, 1147, 310, 12744, 534, 2634, 21399, 64, 17455, 42702, 1159, 310, 1146, 6289, 281, 347, 627, 310, 642, 824, 1159, 2931, 275, 418, 4988, 74, 457, 84, 15548, 6335, 3802, 46588, 15, 7764, 2085, 625, 1491, 390, 3634, 281, 3662, 253, 1953, 13613, 15], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [10795, 253, 2634, 21399, 64, 17455, 42702, 1159, 22065, 667, 7269, 3733, 4610, 824, 347, 4715, 2281, 28631, 390, 2393, 15910, 32, 1147, 310, 12744, 534, 2634, 21399, 64, 17455, 42702, 1159, 310, 1146, 6289, 281, 347, 627, 310, 642, 824, 1159, 2931, 275, 418, 4988, 74, 457, 84, 15548, 6335, 3802, 46588, 15, 7764, 2085, 625, 1491, 390, 3634, 281, 3662, 253, 1953, 13613, 15]} (2024-04-04 20:41:18; 2534706368.py:7)\n",
      "INFO:__main__: {'question': 'Does the `add_data()` function support different data augmentation techniques or preprocessing options for training data?', 'answer': \"No, the `add_data()` function does not support different data augmentation techniques or preprocessing options for training data. It simply adds the provided examples to the program's list of examples.\", 'input_ids': [10795, 253, 2634, 1911, 64, 2203, 42702, 1159, 1329, 1027, 941, 42072, 5609, 390, 638, 21678, 4610, 323, 3733, 941, 32, 2302, 13, 253, 2634, 1911, 64, 2203, 42702, 1159, 1057, 417, 1329, 1027, 941, 42072, 5609, 390, 638, 21678, 4610, 323, 3733, 941, 15, 733, 3365, 11323, 253, 2530, 6667, 281, 253, 2086, 434, 1618, 273, 6667, 15], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [10795, 253, 2634, 1911, 64, 2203, 42702, 1159, 1329, 1027, 941, 42072, 5609, 390, 638, 21678, 4610, 323, 3733, 941, 32, 2302, 13, 253, 2634, 1911, 64, 2203, 42702, 1159, 1057, 417, 1329, 1027, 941, 42072, 5609, 390, 638, 21678, 4610, 323, 3733, 941, 15, 733, 3365, 11323, 253, 2530, 6667, 281, 253, 2086, 434, 1618, 273, 6667, 15]} (2024-04-04 20:41:18; 2534706368.py:7)\n",
      "INFO:__main__: {'question': 'Can Lamini generate text for data storytelling or data visualization purposes?', 'answer': 'Yes, Lamini can generate text for data storytelling or data visualization purposes using its natural language generation capabilities.', 'input_ids': [5804, 418, 4988, 74, 6635, 2505, 323, 941, 41651, 390, 941, 24426, 6378, 32, 4374, 13, 418, 4988, 74, 476, 6635, 2505, 323, 941, 41651, 390, 941, 24426, 6378, 970, 697, 3626, 3448, 5978, 13789, 15], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [5804, 418, 4988, 74, 6635, 2505, 323, 941, 41651, 390, 941, 24426, 6378, 32, 4374, 13, 418, 4988, 74, 476, 6635, 2505, 323, 941, 41651, 390, 941, 24426, 6378, 970, 697, 3626, 3448, 5978, 13789, 15]} (2024-04-04 20:41:18; 2534706368.py:7)\n",
      "INFO:__main__: {'question': 'Can the documentation predict the outcome of a coin toss?', 'answer': 'I wish! This documentation only talks about Lamini’s LLM Engine.', 'input_ids': [5804, 253, 10097, 3283, 253, 6454, 273, 247, 18011, 15331, 32, 42, 5730, 2, 831, 10097, 760, 12088, 670, 418, 4988, 74, 457, 84, 21708, 46, 10797, 15], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [5804, 253, 10097, 3283, 253, 6454, 273, 247, 18011, 15331, 32, 42, 5730, 2, 831, 10097, 760, 12088, 670, 418, 4988, 74, 457, 84, 21708, 46, 10797, 15]} (2024-04-04 20:41:18; 2534706368.py:7)\n",
      "INFO:__main__: {'question': 'How does the `submit_job()` function work in Lamini? What does it mean to submit a job, and what happens behind the scenes?', 'answer': 'The `submit_job()` function in Lamini is used to submit a job for training a machine learning model. When you submit a job, Lamini takes the training data and uses it to train a model based on the specified parameters. Behind the scenes, Lamini uses distributed computing to train the model on multiple machines, which allows for faster training times. Once the training is complete, the resulting model is saved and can be used for inference.', 'input_ids': [2347, 1057, 253, 2634, 21399, 64, 17455, 42702, 1159, 789, 275, 418, 4988, 74, 32, 1737, 1057, 352, 1599, 281, 11929, 247, 2628, 13, 285, 752, 6569, 3212, 253, 13451, 32, 510, 2634, 21399, 64, 17455, 42702, 1159, 275, 418, 4988, 74, 310, 908, 281, 11929, 247, 2628, 323, 3733, 247, 5145, 4715, 1566, 15, 2091, 368, 11929, 247, 2628, 13, 418, 4988, 74, 3936, 253, 3733, 941, 285, 4648, 352, 281, 6194, 247, 1566, 1754, 327, 253, 7616, 3602, 15, 33341, 253, 13451, 13, 418, 4988, 74, 4648, 5939, 12672, 281, 6194, 253, 1566, 327, 2709, 10679, 13, 534, 4483, 323, 7938, 3733, 2069, 15, 7243, 253, 3733, 310, 3426, 13, 253, 4795, 1566, 310, 9809, 285, 476, 320, 908, 323, 17032, 15], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [2347, 1057, 253, 2634, 21399, 64, 17455, 42702, 1159, 789, 275, 418, 4988, 74, 32, 1737, 1057, 352, 1599, 281, 11929, 247, 2628, 13, 285, 752, 6569, 3212, 253, 13451, 32, 510, 2634, 21399, 64, 17455, 42702, 1159, 275, 418, 4988, 74, 310, 908, 281, 11929, 247, 2628, 323, 3733, 247, 5145, 4715, 1566, 15, 2091, 368, 11929, 247, 2628, 13, 418, 4988, 74, 3936, 253, 3733, 941, 285, 4648, 352, 281, 6194, 247, 1566, 1754, 327, 253, 7616, 3602, 15, 33341, 253, 13451, 13, 418, 4988, 74, 4648, 5939, 12672, 281, 6194, 253, 1566, 327, 2709, 10679, 13, 534, 4483, 323, 7938, 3733, 2069, 15, 7243, 253, 3733, 310, 3426, 13, 253, 4795, 1566, 310, 9809, 285, 476, 320, 908, 323, 17032, 15]} (2024-04-04 20:41:18; 2534706368.py:7)\n",
      "INFO:__main__: {'question': 'Does Lamini support generating code', 'answer': 'Yes, Lamini supports generating code through its API.', 'input_ids': [10795, 418, 4988, 74, 1329, 11365, 2127, 4374, 13, 418, 4988, 74, 8525, 11365, 2127, 949, 697, 8990, 15], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [10795, 418, 4988, 74, 1329, 11365, 2127, 4374, 13, 418, 4988, 74, 8525, 11365, 2127, 949, 697, 8990, 15]} (2024-04-04 20:41:18; 2534706368.py:7)\n",
      "INFO:__main__: {'question': 'Can Lamini be used to create chatbots or virtual assistants?', 'answer': 'Yes, Lamini can be used to build conversational AI agents or chatbots. It provides tools and functionalities for generating coherent and contextually appropriate responses in conversational settings, as well as support for multi-turn conversations and context-aware recommendation systems.', 'input_ids': [5804, 418, 4988, 74, 320, 908, 281, 2794, 12939, 67, 1502, 390, 7503, 35785, 32, 4374, 13, 418, 4988, 74, 476, 320, 908, 281, 1973, 5636, 1050, 14980, 6083, 390, 12939, 67, 1502, 15, 733, 3400, 5657, 285, 5164, 1005, 323, 11365, 18893, 285, 3634, 1230, 4569, 6128, 275, 5636, 1050, 7533, 13, 347, 973, 347, 1329, 323, 4471, 14, 14077, 16072, 285, 3634, 14, 13823, 17401, 2718, 15], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [5804, 418, 4988, 74, 320, 908, 281, 2794, 12939, 67, 1502, 390, 7503, 35785, 32, 4374, 13, 418, 4988, 74, 476, 320, 908, 281, 1973, 5636, 1050, 14980, 6083, 390, 12939, 67, 1502, 15, 733, 3400, 5657, 285, 5164, 1005, 323, 11365, 18893, 285, 3634, 1230, 4569, 6128, 275, 5636, 1050, 7533, 13, 347, 973, 347, 1329, 323, 4471, 14, 14077, 16072, 285, 3634, 14, 13823, 17401, 2718, 15]} (2024-04-04 20:41:18; 2534706368.py:7)\n"
     ]
    }
   ],
   "source": [
    "# Loadign the lamini dataset from hugging face\n",
    "finetuning_dataset_path = \"lamini/lamini_docs\"\n",
    "finetuning_dataset = datasets.load_dataset(finetuning_dataset_path)\n",
    "m = 10\n",
    "top_m = list(itertools.islice(finetuning_dataset['train'], m))\n",
    "for i in top_m:\n",
    "    logger.info(i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
