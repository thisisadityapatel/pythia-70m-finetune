{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-Tuning a small LLM (pythia - 410 Million Parameters with the lamini company finetuning dataset from HugginFace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import tempfile\n",
    "import logging\n",
    "import random\n",
    "import config\n",
    "import os\n",
    "import yaml\n",
    "import time\n",
    "import torch\n",
    "import transformers\n",
    "import pandas as pd\n",
    "import jsonlines\n",
    "import sys\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForCausalLM\n",
    "from transformers import TrainingArguments\n",
    "from transformers import AutoModelForCausalLM\n",
    "from transformers import Trainer\n",
    "from llama import BasicModelRunner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up logging\n",
    "logger = logging.getLogger(__name__)\n",
    "def setup_logging():\n",
    "    FORMAT = '%(levelname)s:%(name)s: %(message)s (%(asctime)s; %(filename)s:%(lineno)d)'\n",
    "    DATE_FORMAT = '%Y-%m-%d %H:%M:%S'\n",
    "    LEVEL = logging.INFO\n",
    "    STREAM = sys.stdout\n",
    "    logging.basicConfig( \n",
    "        level=LEVEL, \n",
    "        format=FORMAT, \n",
    "        datefmt=DATE_FORMAT,\n",
    "        stream=STREAM,\n",
    "    )\n",
    "setup_logging()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"lamini/lamini_docs\"\n",
    "model_name = \"EleutherAI/pythia-70m\"\n",
    "using_hugginface = True\n",
    "\n",
    "# Loading the dataset and the train-test datasets\n",
    "finetuning_dataset = datasets.load_dataset(dataset_path)\n",
    "training_dataset, test_datastet = finetuning_dataset['train'], finetuning_dataset['test']\n",
    "\n",
    "# Loading the model\n",
    "base_model = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CPU and GPU device presence and loading the base_model location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:__main__: cpu (2024-04-04 20:39:18; 1956756189.py:9)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPTNeoXForCausalLM(\n",
       "  (gpt_neox): GPTNeoXModel(\n",
       "    (embed_in): Embedding(50304, 512)\n",
       "    (emb_dropout): Dropout(p=0.0, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-5): 6 x GPTNeoXLayer(\n",
       "        (input_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (attention): GPTNeoXAttention(\n",
       "          (rotary_emb): GPTNeoXRotaryEmbedding()\n",
       "          (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (mlp): GPTNeoXMLP(\n",
       "          (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (act): GELUActivation()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (embed_out): Linear(in_features=512, out_features=50304, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device_count = torch.cuda.device_count()\n",
    "if device_count > 0:\n",
    "    print('Select GPU device')\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    logger.debug(\"Select CPU device\")\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "logger.info(device)\n",
    "base_model.to(device) # Loading the model onto the GPU/CPU depending on the availability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inference function to prompt the LLM hosted locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:__main__: \n",
      "\n",
      "I'm not sure if I'm going to be in the same place as you, but I'm not sure if I'm going to be in the same place as you, but I'm not sure if I'm going to be in the same place as you, but I'm not sure if I'm going to be in the same place as you, but I'm not sure if I'm going to be in the same place as you, (2024-04-04 20:39:19; 3471472043.py:30)\n"
     ]
    }
   ],
   "source": [
    "def inference(text, model, tokenizer, max_input_tokens=1000, max_output_tokens=100) -> str:\n",
    "    # Encoding\n",
    "    input_ids = tokenizer.encode(\n",
    "        text,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=max_input_tokens\n",
    "    )\n",
    "\n",
    "    # Generate\n",
    "    device = model.device\n",
    "    generated_prompt_with_text = model.generate(\n",
    "        input_ids=input_ids.to(device), # The tokens need to be put on the same device at the LLM (GPU or CPU)\n",
    "        max_length=max_output_tokens\n",
    "    )\n",
    "\n",
    "    # Decoding\n",
    "    generated_prompt_with_tokens = tokenizer.batch_decode( # batch_decode for decoding multiple token lists all-together\n",
    "        generated_prompt_with_text,\n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "\n",
    "    # Trimming the initial Question out of the answer\n",
    "    generated_text_answer = generated_prompt_with_tokens[0][len(text): ]\n",
    "\n",
    "    return generated_text_answer\n",
    "\n",
    "# Testing a prompt\n",
    "# Probably halucinated answer\n",
    "logger.info(inference(\"Hey what's the weather in toronto?\", base_model, tokenizer))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_steps = 5\n",
    "\n",
    "trained_model_name = f\"lamini_docs_{max_steps}_steps\"\n",
    "output_dir = trained_model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "\n",
    "  # Learning rate\n",
    "  learning_rate=1.0e-5,\n",
    "\n",
    "  # Number of training epochs\n",
    "  num_train_epochs=1,\n",
    "\n",
    "  # Max steps to train for (each step is a batch of data)\n",
    "  # Overrides num_train_epochs, if not -1\n",
    "  max_steps=max_steps,\n",
    "\n",
    "  # Batch size for training\n",
    "  per_device_train_batch_size=1,\n",
    "\n",
    "  # Directory to save model checkpoints\n",
    "  output_dir=output_dir,\n",
    "\n",
    "  # Other arguments\n",
    "  overwrite_output_dir=False, # Overwrite the content of the output directory\n",
    "  disable_tqdm=False, # Disable progress bars\n",
    "  eval_steps=120, # Number of update steps between two evaluations\n",
    "  save_steps=120, # After # steps model is saved\n",
    "  warmup_steps=1, # Number of warmup steps for learning rate scheduler\n",
    "  per_device_eval_batch_size=1, # Batch size for evaluation\n",
    "  evaluation_strategy=\"steps\",\n",
    "  logging_strategy=\"steps\",\n",
    "  logging_steps=1,\n",
    "  optim=\"adafactor\",\n",
    "  gradient_accumulation_steps = 4,\n",
    "  gradient_checkpointing=False,\n",
    "\n",
    "  # Parameters for early stopping\n",
    "  load_best_model_at_end=True,\n",
    "  save_total_limit=1,\n",
    "  metric_for_best_model=\"eval_loss\",\n",
    "  greater_is_better=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Monitor memory footprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:__main__: GPTNeoXForCausalLM(\n",
      "  (gpt_neox): GPTNeoXModel(\n",
      "    (embed_in): Embedding(50304, 512)\n",
      "    (emb_dropout): Dropout(p=0.0, inplace=False)\n",
      "    (layers): ModuleList(\n",
      "      (0-5): 6 x GPTNeoXLayer(\n",
      "        (input_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (post_attention_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "        (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
      "        (attention): GPTNeoXAttention(\n",
      "          (rotary_emb): GPTNeoXRotaryEmbedding()\n",
      "          (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
      "          (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (mlp): GPTNeoXMLP(\n",
      "          (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (act): GELUActivation()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (embed_out): Linear(in_features=512, out_features=50304, bias=False)\n",
      ") (2024-04-04 20:39:19; 2226608322.py:12)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/logging/__init__.py\", line 1083, in emit\n",
      "    msg = self.format(record)\n",
      "  File \"/Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/logging/__init__.py\", line 927, in format\n",
      "    return fmt.format(record)\n",
      "  File \"/Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/logging/__init__.py\", line 663, in format\n",
      "    record.message = record.getMessage()\n",
      "  File \"/Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/logging/__init__.py\", line 367, in getMessage\n",
      "    msg = msg % self.args\n",
      "TypeError: not all arguments converted during string formatting\n",
      "Call stack:\n",
      "  File \"/Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/Users/adityapatel/Desktop/pythia-70m-finetuning/.venv/lib/python3.9/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/Users/adityapatel/Desktop/pythia-70m-finetuning/.venv/lib/python3.9/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/Users/adityapatel/Desktop/pythia-70m-finetuning/.venv/lib/python3.9/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/Users/adityapatel/Desktop/pythia-70m-finetuning/.venv/lib/python3.9/site-packages/tornado/platform/asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/asyncio/base_events.py\", line 596, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/asyncio/base_events.py\", line 1890, in _run_once\n",
      "    handle._run()\n",
      "  File \"/Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/Users/adityapatel/Desktop/pythia-70m-finetuning/.venv/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/Users/adityapatel/Desktop/pythia-70m-finetuning/.venv/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/Users/adityapatel/Desktop/pythia-70m-finetuning/.venv/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"/Users/adityapatel/Desktop/pythia-70m-finetuning/.venv/lib/python3.9/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/Users/adityapatel/Desktop/pythia-70m-finetuning/.venv/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/Users/adityapatel/Desktop/pythia-70m-finetuning/.venv/lib/python3.9/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/Users/adityapatel/Desktop/pythia-70m-finetuning/.venv/lib/python3.9/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/Users/adityapatel/Desktop/pythia-70m-finetuning/.venv/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3048, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/Users/adityapatel/Desktop/pythia-70m-finetuning/.venv/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3103, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/Users/adityapatel/Desktop/pythia-70m-finetuning/.venv/lib/python3.9/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/Users/adityapatel/Desktop/pythia-70m-finetuning/.venv/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3308, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/Users/adityapatel/Desktop/pythia-70m-finetuning/.venv/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3490, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/Users/adityapatel/Desktop/pythia-70m-finetuning/.venv/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3550, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/v1/jdh6fxf142q38l__7b53ncq00000gn/T/ipykernel_16873/2226608322.py\", line 13, in <module>\n",
      "    logger.info(\"Memory footprint\", base_model.get_memory_footprint() / 1e9, \"GB\")\n",
      "Message: 'Memory footprint'\n",
      "Arguments: (0.3084454, 'GB')\n",
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/logging/__init__.py\", line 1083, in emit\n",
      "    msg = self.format(record)\n",
      "  File \"/Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/logging/__init__.py\", line 927, in format\n",
      "    return fmt.format(record)\n",
      "  File \"/Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/logging/__init__.py\", line 663, in format\n",
      "    record.message = record.getMessage()\n",
      "  File \"/Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/logging/__init__.py\", line 367, in getMessage\n",
      "    msg = msg % self.args\n",
      "TypeError: not all arguments converted during string formatting\n",
      "Call stack:\n",
      "  File \"/Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/Users/adityapatel/Desktop/pythia-70m-finetuning/.venv/lib/python3.9/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/Users/adityapatel/Desktop/pythia-70m-finetuning/.venv/lib/python3.9/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/Users/adityapatel/Desktop/pythia-70m-finetuning/.venv/lib/python3.9/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/Users/adityapatel/Desktop/pythia-70m-finetuning/.venv/lib/python3.9/site-packages/tornado/platform/asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/asyncio/base_events.py\", line 596, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/asyncio/base_events.py\", line 1890, in _run_once\n",
      "    handle._run()\n",
      "  File \"/Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/Users/adityapatel/Desktop/pythia-70m-finetuning/.venv/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/Users/adityapatel/Desktop/pythia-70m-finetuning/.venv/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/Users/adityapatel/Desktop/pythia-70m-finetuning/.venv/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"/Users/adityapatel/Desktop/pythia-70m-finetuning/.venv/lib/python3.9/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/Users/adityapatel/Desktop/pythia-70m-finetuning/.venv/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/Users/adityapatel/Desktop/pythia-70m-finetuning/.venv/lib/python3.9/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/Users/adityapatel/Desktop/pythia-70m-finetuning/.venv/lib/python3.9/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/Users/adityapatel/Desktop/pythia-70m-finetuning/.venv/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3048, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/Users/adityapatel/Desktop/pythia-70m-finetuning/.venv/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3103, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/Users/adityapatel/Desktop/pythia-70m-finetuning/.venv/lib/python3.9/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/Users/adityapatel/Desktop/pythia-70m-finetuning/.venv/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3308, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/Users/adityapatel/Desktop/pythia-70m-finetuning/.venv/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3490, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/Users/adityapatel/Desktop/pythia-70m-finetuning/.venv/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3550, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/v1/jdh6fxf142q38l__7b53ncq00000gn/T/ipykernel_16873/2226608322.py\", line 14, in <module>\n",
      "    logger.info(\"Flops\", model_flops / 1e9, \"GFLOPs\")\n",
      "Message: 'Flops'\n",
      "Arguments: (2195.667812352, 'GFLOPs')\n"
     ]
    }
   ],
   "source": [
    "model_flops = (\n",
    "  base_model.floating_point_ops(\n",
    "    {\n",
    "       \"input_ids\": torch.zeros(\n",
    "           (1, 2048)\n",
    "      )\n",
    "    }\n",
    "  )\n",
    "  * training_args.gradient_accumulation_steps\n",
    ")\n",
    "\n",
    "logger.info(base_model)\n",
    "logger.info(\"Memory footprint\", base_model.get_memory_footprint() / 1e9, \"GB\")\n",
    "logger.info(\"Flops\", model_flops / 1e9, \"GFLOPs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=base_model,\n",
    "    args=training_args,\n",
    "    train_dataset=training_dataset,\n",
    "    eval_dataset=test_datastet,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initiating the Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93524d8c4589412ea64d2787af72e97d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.1562, 'grad_norm': 76.75056457519531, 'learning_rate': 1e-05, 'epoch': 0.0}\n",
      "{'loss': 3.0687, 'grad_norm': 56.96006774902344, 'learning_rate': 7.500000000000001e-06, 'epoch': 0.01}\n",
      "{'loss': 3.8929, 'grad_norm': 54.13835144042969, 'learning_rate': 5e-06, 'epoch': 0.01}\n",
      "{'loss': 3.4545, 'grad_norm': 49.51322555541992, 'learning_rate': 2.5e-06, 'epoch': 0.01}\n",
      "{'loss': 3.1985, 'grad_norm': 39.19015884399414, 'learning_rate': 0.0, 'epoch': 0.02}\n",
      "{'train_runtime': 59.5776, 'train_samples_per_second': 0.336, 'train_steps_per_second': 0.084, 'train_loss': 3.5541662216186523, 'epoch': 0.02}\n"
     ]
    }
   ],
   "source": [
    "training_output = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving the model locally in the pre-set directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = f'{output_dir}/final_finetuned_model'\n",
    "trainer.save_model(save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running the fine-tuned locally saved model     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:__main__: \n",
      "\n",
      "I'm not sure if I'm going to be in the same boat, but I'm not sure if I'm going to be in the same boat.\n",
      "\n",
      "I'm not sure if I'm going to be in the same boat, but I'm not sure if I'm going to be in the same boat.\n",
      "\n",
      "I'm not sure if I'm going to be in the same boat, but I'm not sure if I'm (2024-04-04 20:42:31; 509760181.py:5)\n"
     ]
    }
   ],
   "source": [
    "finetuned_slightly_model = AutoModelForCausalLM.from_pretrained(save_dir, local_files_only=True)\n",
    "finetuned_slightly_model.to(device)\n",
    "\n",
    "# Probably halucinated answer, since it is trained only on 5 data points but with more training you can gauge the accuracy of the response\n",
    "logger.info(inference(\"Hey what's the weather in toronto?\", finetuned_slightly_model, tokenizer))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
